#         countryCode = countryCode,
#         Year.ini = Year.ini.x,
#         Year.end = Year.end.y,
#         GDP.t0 = GDP.t0.x,
#         LE.t0 = LE.t0.x,
#         GDP.t1 = GDP.t1.y,
#         LE.t1 = LE.t1.y
#     )
#
# # Filter to get non-overlapping 10-year intervals (e.g., 1960-1970, 1970-1980)
# startYearOfDecade = min(data_10y$Year.ini) %% 10
# data = data_10y %>% filter(Year.ini %% 10 == startYearOfDecade)
data = data %>% mutate(GDP = log(GDP.t0), Delta = (log(GDP.t1)-log(GDP.t0))/5) %>%
select(countryCode, Year = Year.ini, GDP, Delta)
data0 = filter(data, Year < max(data$Year)) %>% arrange(Year,countryCode)
data1 = filter(data, Year > min(data$Year)) %>% arrange(Year,countryCode)
dataYearIni = filter(data, Year == min(data$Year))
dataYearEnd = filter(data, Year == max(data$Year))
# regressione
plot(data$GDP,data$Delta,xlab="logGDP",ylab="Delta logGDP 10Anni")
sm.regression(data$GDP,data$Delta)
X0 = cbind(data0$GDP,data0$Delta)
X1 = cbind(data1$GDP,data1$Delta)
# parameters ----
nEval = 2500
xGrid = seq(from=0.9*min(c(X0[,1],X1[,1])), to=1.1*max(c(X0[,1],X1[,1])), length.out=round(sqrt(nEval)))
yGrid = seq(from=0.9*min(c(X0[,2],X1[,2])), to=1.1*max(c(X0[,2],X1[,2])), length.out=round(sqrt(nEval)))
x = as.matrix(expand.grid(xGrid, yGrid))
est_field_adaptive = NWfieldAdaptive(X0, X1, x=x, kernel.type="epa",method.h = "silverman",
chunk_size=1000,
sparse=FALSE, gc=TRUE,
hOpt = TRUE, alphaOpt = TRUE)
# est_field_adaptive = NWfieldAdaptive(X0, X1, x=x, kernel.type="epa",
#                                      chunk_size=1000,h=0.1,
#                                      sparse=FALSE, gc=TRUE,
#                                      hOpt = FALSE, alphaOpt = FALSE)
est_field_adaptive_bootstrap = bootstrapKernelFieldErrors(est_field_adaptive, B = 100)
signifBoot = significanceBootstrap(est_field_adaptive,est_field_adaptive_bootstrap)
signifInd = which(signifBoot)
dev.new()
op <- par(family = "mono")
lengthArrows=0.2
plot(x, type = "n", xlab="logGDP",ylab="Delta logGDP 10 Anni", main = "Solo significative",ylim=c(-0.05,0.075))
points(dataYearIni$GDP,dataYearIni$Delta,col="blue",pch=19,cex=.45)
text(dataYearIni$GDP,dataYearIni$Delta,dataYearIni$countryCode,col="blue",pos=4,cex=.45)
sm.reg = sm.regression(data$GDP,data$Delta,display="none")
lines(sm.reg$eval.points,sm.reg$estimate,col="purple")
arrows(est_field_adaptive$x[signifInd,1], est_field_adaptive$x[signifInd,2],
est_field_adaptive$x[signifInd,1] + lengthArrows*est_field_adaptive$estimator[signifInd,1],
est_field_adaptive$x[signifInd,2] + lengthArrows*est_field_adaptive$estimator[signifInd,2],
length = 0.05, angle = 15, col = "black")
abline(h=mean(data$Delta))
abline(h=0)
abline(v=0)
par(op)
dev.copy2pdf(file="fiaschilavezziSignif.pdf")
## plot campo stimato significativo ----
signifInd = which(signifBoot)
dev.new()
op <- par(family = "mono")
lengthArrows=0.2
plot(x, type = "n", xlab="logGDP",ylab="Delta logGDP 10 Anni", main = "Solo significative",ylim=c(-0.05,0.075))
points(dataYearIni$GDP,dataYearIni$Delta,col="blue",pch=19,cex=.45)
text(dataYearIni$GDP,dataYearIni$Delta,dataYearIni$countryCode,col="blue",pos=4,cex=.45)
sm.reg = sm.regression(data$GDP,data$Delta,display="none")
lines(sm.reg$eval.points,sm.reg$estimate,col="purple")
arrows(est_field_adaptive$x[signifInd,1], est_field_adaptive$x[signifInd,2],
est_field_adaptive$x[signifInd,1] + 10*lengthArrows*est_field_adaptive$estimator[signifInd,1],
est_field_adaptive$x[signifInd,2] + lengthArrows*est_field_adaptive$estimator[signifInd,2],
length = 0.05, angle = 15, col = "black")
abline(h=mean(data$Delta))
abline(h=0)
abline(v=0)
par(op)
## plot campo stimato significativo ----
signifInd = which(signifBoot)
dev.new()
op <- par(family = "mono")
lengthArrows=0.2
plot(x, type = "n", xlab="logGDP",ylab="Delta logGDP 10 Anni", main = "Solo significative",ylim=c(-0.05,0.075))
points(dataYearIni$GDP,dataYearIni$Delta,col="blue",pch=19,cex=.45)
text(dataYearIni$GDP,dataYearIni$Delta,dataYearIni$countryCode,col="blue",pos=4,cex=.45)
sm.reg = sm.regression(data$GDP,data$Delta,display="none")
lines(sm.reg$eval.points,sm.reg$estimate,col="purple")
arrows(est_field_adaptive$x[signifInd,1], est_field_adaptive$x[signifInd,2],
est_field_adaptive$x[signifInd,1] + lengthArrows*est_field_adaptive$estimator[signifInd,1],
est_field_adaptive$x[signifInd,2] + lengthArrows*est_field_adaptive$estimator[signifInd,2],
length = 0.05, angle = 15, col = "black")
abline(h=mean(data$Delta))
abline(h=0)
abline(v=0)
par(op)
dev.copy2pdf(file="fiaschilavezziSignif.pdf")
hilbert <- function(n) {
i <- 1:n
j <- 1:n
1 / (outer(i - 1, j, "+"))
}
A <- hilbert(10)
set.seed(42)
n <- 10
A <- hilbert(n)
x_true <- 1:n
b <- A %*% x_true
# This will likely throw a warning
x_naive <- solve(A, b)
#> Warning in solve.default(A, b): system is computationally singular:
#> reciprocal condition number = 6.2402e-14
# Compare with the true solution
print(x_naive)
#>  [1]  1.000000  1.999999  3.000006  3.999978  5.000050  5.999912  7.000096
#>  [8]  7.999914  9.000030 10.000010
# Calculate the error
sum((x_naive - x_true)^2)
#> [1] 2.193136e-08
# Perform SVD
svd_A <- svd(A)
U <- svd_A$u
D <- diag(svd_A$d)
V <- svd_A$v
# The singular values show the problem - they get incredibly small
print(svd_A$d)
#>  [1] 1.664468e+00 2.871578e-01 2.305413e-02 1.134737e-03 3.655890e-05
#>  [6] 7.978254e-07 1.171866e-08 1.116035e-10 6.641618e-13 2.302307e-15
# Compute the pseudoinverse of A by inverting non-tiny singular values
# Set a tolerance to avoid dividing by nearly zero
tol <- .Machine$double.eps
d_inv <- 1 / svd_A$d
d_inv[svd_A$d < tol] <- 0 # Crucial step: treat tiny values as zero
D_inv <- diag(d_inv)
# A+ = V D+ U'
A_pinv <- V %*% D_inv %*% t(U)
# The solution is x = A+ b
x_svd <- A_pinv %*% b
# Compare with the true solution
print(x_svd)
#>            [,1]
#>  [1,]  1.000000
#>  [2,]  2.000000
#>  [3,]  3.000000
#>  [4,]  4.000000
#>  [5,]  5.000000
#>  [6,]  6.000000
#>  [7,]  7.000000
#>  [8,]  8.000000
#>  [9,]  9.000000
#> [10,] 10.000000
# Calculate the error
sum((x_svd - x_true)^2)
A
MASS::ginv(A) %*% b
x_MASS = MASS::ginv(A) %*% b
sum((x_MASS - x_true)^2)
set.seed(42)
n <- 10
A <- hilbert(n)
x_true <- 1:n
b <- A %*% x_true
# Perform SVD
svd_A <- svd(A)
U <- svd_A$u
D <- diag(svd_A$d)
V <- svd_A$v
# The singular values show the problem - they get incredibly small
print(svd_A$d)
#>  [1] 1.664468e+00 2.871578e-01 2.305413e-02 1.134737e-03 3.655890e-05
#>  [6] 7.978254e-07 1.171866e-08 1.116035e-10 6.641618e-13 2.302307e-15
# Compute the pseudoinverse of A by inverting non-tiny singular values
# Set a tolerance to avoid dividing by nearly zero
tol <- .Machine$double.eps
d_inv <- 1 / svd_A$d
d_inv[svd_A$d < tol] <- 0 # Crucial step: treat tiny values as zero
D_inv <- diag(d_inv)
# A+ = V D+ U'
A_pinv <- V %*% D_inv %*% t(U)
# The solution is x = A+ b
x_svd <- A_pinv %*% b
# Compare with the true solution
print(x_svd)
#>            [,1]
#>  [1,]  1.000000
#>  [2,]  2.000000
#>  [3,]  3.000000
#>  [4,]  4.000000
#>  [5,]  5.000000
#>  [6,]  6.000000
#>  [7,]  7.000000
#>  [8,]  8.000000
#>  [9,]  9.000000
#> [10,] 10.000000
# Calculate the error
sum((x_svd - x_true)^2)
#> [1] 3.597951e-25
x_MASS = MASS::ginv(A) %*% b
sum((x_MASS - x_true)^2)
lambda <- 1e-10
# Create the identity matrix
I <- diag(n)
# Solve the regularized system
# The matrix (A + lambda*I) will now be well-conditioned
A_reg <- A + lambda * I
x_ridge <- solve(A_reg, b) # Can use solve() here because A_reg is stable
# Compare with the true solution
print(x_ridge)
#>  [1]  1.000000  2.000000  3.000000  4.000000  5.000000  6.000000  7.000000
#>  [8]  8.000000  9.000000 10.000000
# Calculate the error
sum((x_ridge - x_true)^2)
x_qr <- qr.solve(A, b)
det(A)
cond(A)
kappa(airquality)
kappa(A)
qr.solve(A)
.Machine$double.eps
.Machine$double.eps = 1e-32
.Machine$double.eps
# Perform SVD
svd_A <- svd(A)
U <- svd_A$u
D <- diag(svd_A$d)
V <- svd_A$v
# The singular values show the problem - they get incredibly small
print(svd_A$d)
#>  [1] 1.664468e+00 2.871578e-01 2.305413e-02 1.134737e-03 3.655890e-05
#>  [6] 7.978254e-07 1.171866e-08 1.116035e-10 6.641618e-13 2.302307e-15
# Compute the pseudoinverse of A by inverting non-tiny singular values
# Set a tolerance to avoid dividing by nearly zero
tol <- .Machine$double.eps
d_inv <- 1 / svd_A$d
d_inv[svd_A$d < tol] <- 0 # Crucial step: treat tiny values as zero
D_inv <- diag(d_inv)
# A+ = V D+ U'
A_pinv <- V %*% D_inv %*% t(U)
# The solution is x = A+ b
x_svd <- A_pinv %*% b
# Compare with the true solution
print(x_svd)
#>            [,1]
#>  [1,]  1.000000
#>  [2,]  2.000000
#>  [3,]  3.000000
#>  [4,]  4.000000
#>  [5,]  5.000000
#>  [6,]  6.000000
#>  [7,]  7.000000
#>  [8,]  8.000000
#>  [9,]  9.000000
#> [10,] 10.000000
# Calculate the error
sum((x_svd - x_true)^2)
lambda <- 1e-10
I <- diag(n)
# Risolvere il sistema regolarizzato
# Ora la matrice (A + lambda*I) non è più singolare
x_ridge <- solve(A + lambda * I, b)
# L'errore è incredibilmente piccolo, perché la regolarizzazione
# ha stabilizzato il problema in modo molto efficace.
sum((x_ridge - x_true)^2)
#> [1] 2.148419e-25
# Let's create a classic ill-conditioned matrix: the Hilbert matrix
hilbert <- function(n) {
i <- 1:n
j <- 1:n
1 / (outer(i - 1, j, "+"))
}
A <- hilbert(10)
# The condition number will be very large
# kappa() is a good estimate
kappa(A)
set.seed(42)
n <- 10
A <- hilbert(n)
x_true <- 1:n
b <- A %*% x_true
# This will likely throw a warning
x_naive <- solve(A, b)
#> Warning in solve.default(A, b): system is computationally singular:
#> reciprocal condition number = 6.2402e-14
# Compare with the true solution
print(x_naive)
#>  [1]  1.000000  1.999999  3.000006  3.999978  5.000050  5.999912  7.000096
#>  [8]  7.999914  9.000030 10.000010
# Calculate the error
sum((x_naive - x_true)^2)
#> [1] 2.193136e-08
svd_A <- svd(A)
U <- svd_A$u
D <- diag(svd_A$d)
V <- svd_A$v
# The singular values show the problem - they get incredibly small
print(svd_A$d)
#>  [1] 1.664468e+00 2.871578e-01 2.305413e-02 1.134737e-03 3.655890e-05
#>  [6] 7.978254e-07 1.171866e-08 1.116035e-10 6.641618e-13 2.302307e-15
# Compute the pseudoinverse of A by inverting non-tiny singular values
# Set a tolerance to avoid dividing by nearly zero
tol <- .Machine$double.eps
d_inv <- 1 / svd_A$d
d_inv[svd_A$d < tol] <- 0 # Crucial step: treat tiny values as zero
D_inv <- diag(d_inv)
# A+ = V D+ U'
A_pinv <- V %*% D_inv %*% t(U)
# The solution is x = A+ b
x_svd <- A_pinv %*% b
# Compare with the true solution
print(x_svd)
#>            [,1]
#>  [1,]  1.000000
#>  [2,]  2.000000
#>  [3,]  3.000000
#>  [4,]  4.000000
#>  [5,]  5.000000
#>  [6,]  6.000000
#>  [7,]  7.000000
#>  [8,]  8.000000
#>  [9,]  9.000000
#> [10,] 10.000000
# Calculate the error
sum((x_svd - x_true)^2)
x_MASS = MASS::ginv(A) %*% b
sum((x_MASS - x_true)^2)
.Machine$double.eps
rm(list=ls())
.Machine$double.eps
# Let's create a classic ill-conditioned matrix: the Hilbert matrix
hilbert <- function(n) {
i <- 1:n
j <- 1:n
1 / (outer(i - 1, j, "+"))
}
A <- hilbert(10)
# The condition number will be very large
# kappa() is a good estimate
kappa(A)
set.seed(42)
n <- 10
A <- hilbert(n)
x_true <- 1:n
b <- A %*% x_true
# This will likely throw a warning
x_naive <- solve(A, b)
#> Warning in solve.default(A, b): system is computationally singular:
#> reciprocal condition number = 6.2402e-14
# Compare with the true solution
print(x_naive)
#>  [1]  1.000000  1.999999  3.000006  3.999978  5.000050  5.999912  7.000096
#>  [8]  7.999914  9.000030 10.000010
# Calculate the error
sum((x_naive - x_true)^2)
#> [1] 2.193136e-08
svd_A <- svd(A)
U <- svd_A$u
D <- diag(svd_A$d)
V <- svd_A$v
# The singular values show the problem - they get incredibly small
print(svd_A$d)
#>  [1] 1.664468e+00 2.871578e-01 2.305413e-02 1.134737e-03 3.655890e-05
#>  [6] 7.978254e-07 1.171866e-08 1.116035e-10 6.641618e-13 2.302307e-15
# Compute the pseudoinverse of A by inverting non-tiny singular values
# Set a tolerance to avoid dividing by nearly zero
tol <- .Machine$double.eps
d_inv <- 1 / svd_A$d
d_inv[svd_A$d < tol] <- 0 # Crucial step: treat tiny values as zero
D_inv <- diag(d_inv)
# A+ = V D+ U'
A_pinv <- V %*% D_inv %*% t(U)
# The solution is x = A+ b
x_svd <- A_pinv %*% b
# Compare with the true solution
print(x_svd)
#>            [,1]
#>  [1,]  1.000000
#>  [2,]  2.000000
#>  [3,]  3.000000
#>  [4,]  4.000000
#>  [5,]  5.000000
#>  [6,]  6.000000
#>  [7,]  7.000000
#>  [8,]  8.000000
#>  [9,]  9.000000
#> [10,] 10.000000
# Calculate the error
sum((x_svd - x_true)^2)
x_MASS = MASS::ginv(A) %*% b
sum((x_MASS - x_true)^2)
.Machine$double.eps
# Let's create a classic ill-conditioned matrix: the Hilbert matrix
hilbert <- function(n) {
i <- 1:n
j <- 1:n
1 / (outer(i - 1, j, "+"))
}
A <- hilbert(10)
# The condition number will be very large
# kappa() is a good estimate
kappa(A)
set.seed(42)
n <- 10
A <- hilbert(n)
x_true <- 1:n
b <- A %*% x_true
# This will likely throw a warning
x_naive <- solve(A, b)
#> Warning in solve.default(A, b): system is computationally singular:
#> reciprocal condition number = 6.2402e-14
# Compare with the true solution
print(x_naive)
#>  [1]  1.000000  1.999999  3.000006  3.999978  5.000050  5.999912  7.000096
#>  [8]  7.999914  9.000030 10.000010
# Calculate the error
sum((x_naive - x_true)^2)
#> [1] 2.193136e-08
svd_A <- svd(A)
U <- svd_A$u
D <- diag(svd_A$d)
V <- svd_A$v
# The singular values show the problem - they get incredibly small
print(svd_A$d)
#>  [1] 1.664468e+00 2.871578e-01 2.305413e-02 1.134737e-03 3.655890e-05
#>  [6] 7.978254e-07 1.171866e-08 1.116035e-10 6.641618e-13 2.302307e-15
# Compute the pseudoinverse of A by inverting non-tiny singular values
# Set a tolerance to avoid dividing by nearly zero
tol <- .Machine$double.eps
d_inv <- 1 / svd_A$d
d_inv[svd_A$d < tol] <- 0 # Crucial step: treat tiny values as zero
D_inv <- diag(d_inv)
# A+ = V D+ U'
A_pinv <- V %*% D_inv %*% t(U)
# The solution is x = A+ b
x_svd <- A_pinv %*% b
# Compare with the true solution
print(x_svd)
#>            [,1]
#>  [1,]  1.000000
#>  [2,]  2.000000
#>  [3,]  3.000000
#>  [4,]  4.000000
#>  [5,]  5.000000
#>  [6,]  6.000000
#>  [7,]  7.000000
#>  [8,]  8.000000
#>  [9,]  9.000000
#> [10,] 10.000000
# Calculate the error
sum((x_svd - x_true)^2)
x_MASS = MASS::ginv(A) %*% b
sum((x_MASS - x_true)^2)
log10(0.0001218708)
x_qr <- qr.solve(A, b)
qr(A)
?qr.solve
x_qr <- solve(qr(A,LAPACK=TRUE,tol=1e-7),b)
# Compare with the true solution
print(x_qr)
#>  [1]  1.000000e+00  2.000000e+00  3.000000e+00  4.000000e+00  5.000000e+00
#>  [6]  6.000000e+00  7.000000e+00  8.000000e+00  9.000000e+00 1.000000e+01
# Calculate the error
sum((x_qr - x_true)^2)
#> [1] 1.053805e-24
x_qr <- solve(qr(A,LAPACK=TRUE,tol=1e-10),b)
# Compare with the true solution
print(x_qr)
#>  [1]  1.000000e+00  2.000000e+00  3.000000e+00  4.000000e+00  5.000000e+00
#>  [6]  6.000000e+00  7.000000e+00  8.000000e+00  9.000000e+00 1.000000e+01
# Calculate the error
sum((x_qr - x_true)^2)
#> [1] 1.053805e-24
# Let's create a classic ill-conditioned matrix: the Hilbert matrix
hilbert <- function(n) {
i <- 1:n
j <- 1:n
1 / (outer(i - 1, j, "+"))
}
A <- hilbert(10)
# The condition number will be very large
# kappa() is a good estimate
kappa(A)
set.seed(42)
n <- 10
A <- hilbert(n)
x_true <- 1:n
b <- A %*% x_true
# This will likely throw a warning
x_naive <- solve(A, b)
#> Warning in solve.default(A, b): system is computationally singular:
#> reciprocal condition number = 6.2402e-14
# Compare with the true solution
print(x_naive)
#>  [1]  1.000000  1.999999  3.000006  3.999978  5.000050  5.999912  7.000096
#>  [8]  7.999914  9.000030 10.000010
# Calculate the error
sum((x_naive - x_true)^2)
#> [1] 2.193136e-08
set.seed(42)
n <- 10
A <- hilbert(n)
x_true <- 1:n
b <- A %*% x_true
I <- diag(n)
# Testiamo diversi valori di lambda
lambdas <- c(1e-8, 1e-9, 1e-10, 1e-11, 1e-12, 1e-13)
for (l in lambdas) {
A_reg <- A + l * I
# Usiamo un blocco try() per catturare eventuali errori se la matrice diventa troppo singolare
x_ridge <- try(solve(A_reg, b), silent = TRUE)
if (inherits(x_ridge, "try-error")) {
cat(sprintf("Lambda = %e: Il sistema è computazionalmente singolare.\n", l))
} else {
err <- sum((x_ridge - x_true)^2)
cat(sprintf("Lambda = %e: Errore quadratico = %e\n", l, err))
}
kappa(A)
